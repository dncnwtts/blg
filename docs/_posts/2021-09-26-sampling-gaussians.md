---
layout: post
title: "How do I sample from a Gaussian?"
author: "Duncan J. Watts"
categories: journal
tags: [documentation,sample]
<!--image: mountains.jpg-->
---


In Bayesian analysis, something you generally want to do is draw a random number from a distribution and then decide if it's a good fit or not. The first part is harder than it seems. (The second part is easier if you use Gibbs sampling, but that's for another time.)

In general, we have a problem that looks like
$$
\boldsymbol d=\mathsf A\boldsymbol x+\boldsymbol n
$$
where we observe a datastream $\boldsymbol d$ that is generated by some linear process $\mathsf A$ applied to model parameters $\boldsymbol x$, and a noise vector $\boldsymbol n$.

In order to make some sense out of the data, we need a statistical model for the data. We like to assume that the signals and noise are Gaussian distributed, $\boldsymbol x\sim\mathcal N(\boldsymbol\mu,\mathbf S)$ and $\boldsymbol n\sim\mathcal N(\boldsymbol 0,\mathsf N)$.

If we were frequentists, we would ask, "What is the probability of observing this data given a model?"

$$
P(\boldsymbol d\mid\boldsymbol x)=\mathcal N(\mathsf A\boldsymbol x,\mathsf N)
$$

In case this isn't super obvious, we can compute the mean and covariance by hand;
$$
\langle \boldsymbol d\rangle
=\langle\mathsf A\boldsymbol x\rangle+\langle\boldsymbol n\rangle
=\mathsf A\boldsymbol x
$$

since we take $\boldsymbol x$ to be fixed.

$$
\mathsf D=\langle\boldsymbol d\boldsymbol d^T\rangle
-\langle\boldsymbol d\rangle\langle\boldsymbol d\rangle^T
= (\mathsf A\boldsymbol x)(\mathsf A\boldsymbol x)^T
+\langle\boldsymbol n\boldsymbol n^T\rangle-
(\mathsf A\boldsymbol x)(\mathsf A\boldsymbol x)^T
=\mathsf N
$$

Now if we are Bayesian, we could use $P(\boldsymbol x\mid\boldsymbol d)\propto P(\boldsymbol x)P(\boldsymbol d\mid\boldsymbol x)$;

$$
-2\ln P(\boldsymbol x\mid\boldsymbol d)=\boldsymbol x^T\mathsf S^{-1}\boldsymbol x
+(\boldsymbol d-\mathsf A\boldsymbol x)^T\mathsf N^{-1}(\boldsymbol d-\mathsf A\boldsymbol x)
$$

Doing a little bit of algebra, we can show that the mean $\boldsymbol c$ and covariance matrix $\mathsf F$ are given by

$$
\mathsf F^{-1}=\mathsf S^{-1}+\mathsf T^T\mathsf N^{-1}\mathsf T,\qquad
\mathsf F^{-1}\boldsymbol c=\mathsf T^t\mathsf N^{-1}\boldsymbol d
+\mathsf S^{-1}\boldsymbol m
$$

In general, if you want to draw a sample from a multivariate distribution $\boldsymbol x\sim\mathcal N(\boldsymbol m,\mathsf C)$, you can produce it using

$$
\boldsymbol x=\boldsymbol\mu+\mathsf C^{1/2}\boldsymbol \eta
$$

where $\boldsymbol \eta\sim\mathcal N(\boldsymbol 0,\mathsf I)$, which is quite easy to sample, as long as you have a routine to generate uniform random variates, and $\mathsf C^{1/2}$ is some non-unique matrix where $\mathsf C=(\mathsf C^{1/2})(\mathsf C^{1/2})^T$. We can verify this directly;

$$
\langle\boldsymbol x\rangle=\boldsymbol\mu+\mathsf C^{1/2}\langle\boldsymbol\eta\rangle
=\boldsymbol\mu
$$

and

$$
\langle\boldsymbol x\boldsymbol x^T\rangle
=\boldsymbol \mu\boldsymbol \mu^T
+\mathsf C^{1/2}\langle\boldsymbol\eta\boldsymbol\eta^T\rangle(\mathsf C^{1/2})^T
=\langle\boldsymbol\mu\rangle\langle \boldsymbol\mu\rangle^T
+\mathsf C
$$


So in principle, to sample $\boldsymbol x$, we we should just be able to write $\boldsymbol x=\boldsymbol c+\mathsf F^{1/2}\boldsymbol\eta$. The problem with this straightforward approach is that it involves taking the inverse and square root of matrices, both of which are $\mathcal O(n^3)$ operations.

A way to simplify this is by using iterative methods. You can rewrite the direct sampling as $\mathsf F^{-1}\boldsymbol x=\mathsf F^{-1}\boldsymbol c+\mathsf F^{-1/2}\boldsymbol\eta$, or

$$
(\mathsf S^{-1}+\mathsf T^T\mathsf N^{-1}\mathsf T)\boldsymbol x
=\mathsf T^T\mathsf N^{-1}\boldsymbol d+\mathsf S^{-1}\boldsymbol m
+(\mathsf S^{-1}+\mathsf T^T\mathsf N^{-1}\mathsf T)^{1/2}\boldsymbol\eta
$$

If there was no sampling, this would be quite simple. However, taking the square root of this matrix still is not great. However, if we pretend that we were sampling for $\boldsymbol y=(\mathsf S^{-1}+\mathsf T^T\mathsf N^{-1}\mathsf T)\boldsymbol x$, we could write it as $\boldsymbol y=\boldsymbol \mu_y+\mathsf C_y^{1/2}\boldsymbol \eta$, which implies that

$$
\mathsf C_y=\mathsf S^{-1}+\mathsf T^T\mathsf N^{-1}\mathsf T
$$

Now $\boldsymbol y$ can be drawn from a distribution $\mathcal N(\boldsymbol\mu_y,\mathsf C_y)$. This is where something magical happens;

$$
\boldsymbol y\sim\mathcal N(\boldsymbol\mu_y,\mathsf S^{-1}
+\mathsf T^T\mathsf N^{-1}\mathsf T)
$$

can be decomposed into two Gaussian variables, $\boldsymbol y=\boldsymbol y_1+\boldsymbol y_2$, where

$$
\boldsymbol y_1\sim\mathcal N(\boldsymbol\mu_y,\mathsf S^{-1}),\qquad
\boldsybmol y_2\sim\mathcal N(\boldsymbol 0,\mathsf T^T\mathsf N^{-1}\mathsf T)
$$

Now this is much easier to sample from;

$$
\boldsymbol y=\boldsymbol\mu_y+\mathsf S^{-1/2}\boldsymbol\eta_1
+\mathsf T^T\mathsf N^{-1/2}\boldsymbol\eta_2
$$

or, in terms of the variables we were starting with in the first place,

$$
(\mathsf S^{-1}+\mathsf T^T\mathsf N^{-1}\mathsf T)\boldsymbol x
=\mathsf T^T\mathsf N^{-1}\boldsymbol d+\mathsf S^{-1}\boldsymbol m
+\mathsf S^{-1/2}\boldsymbol\eta_1
+\mathsf T^T\mathsf N^{-1/2}\boldsymbol\eta_2
$$

And that, my friends, is how you draw a random sample of a very high-dimensional Gaussian random variable.
