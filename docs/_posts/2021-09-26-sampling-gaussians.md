---
layout: post
title: "How do I sample from a Gaussian?"
author: "Duncan J. Watts"
categories: journal
tags: [documentation,sample]
<!--image: mountains.jpg-->
---


In Bayesian analysis, something you generally want to do is draw a random number from a distribution and then decide if it's a good fit or not. The first part is harder than it seems. (The second part is easier if you use Gibbs sampling, but that's for another time.)

In general, we have a problem that looks like
$$
\boldsymbol d=\mathsf A\boldsymbol x+\boldsymbol n
$$
where we observe a datastream $\boldsymbol d$ that is generated by some linear process $\mathsf A$ applied to model parameters $\boldsymbol x$, and a noise vector $\boldsymbol n$.

In order to make some sense out of the data, we need a statistical model for the data. We like to assume that the signals and noise are Gaussian distributed, $\boldsymbol x\sim\mathcal N(\boldsymbol\mu,\mathbf S)$ and $\boldsymbol n\sim\mathcal N(\boldsymbol 0,\mathsf N)$.

If we were frequentists, we would ask, "What is the probability of observing this data given a model?"

$$
P(\boldsymbol d\mid\boldsymbol x)=\mathcal N(\mathsf A\boldsymbol x,\mathsf N)
$$

In case this isn't super obvious, we can compute the mean and covariance by hand;
$$
\langle \boldsymbol d\rangle
=\langle\mathsf A\boldsymbol x\rangle+\langle\boldsymbol n\rangle
=\mathsf A\boldsymbol x
$$

since we take $\boldsymbol x$ to be fixed.

$$
\mathsf D=\langle\boldsymbol d\boldsymbol d^T\rangle
-\langle\boldsymbol d\rangle\langle\boldsymbol d\rangle^T
= (\mathsf A\boldsymbol x)(\mathsf A\boldsymbol x)^T
+\langle\boldsymbol n\boldsymbol n^T\rangle-
(\mathsf A\boldsymbol x)(\mathsf A\boldsymbol x)^T
=\mathsf N
$$

Now if we are Bayesian, we could use $P(\boldsymbol x\mid\boldsymbol d)\propto P(\boldsymbol x)P(\boldsymbol d\mid\boldsymbol x)$;

$$
-2\ln P(\boldsymbol x\mid\boldsymbol d)=\boldsymbol x^T\mathsf S^{-1}\boldsymbol x
+(\boldsymbol d-\mathsf A\boldsymbol x)^T\mathsf N^{-1}(\boldsymbol d-\mathsf A\boldsymbol x)
$$

Doing a little bit of algebra, we can show that the mean $\boldsymbol c$ and covariance matrix $\mathsf F$ are given by

$$
\mathsf F^{-1}=\mathsf S^{-1}+\mathsf T^T\mathsf N^{-1}\mathsf T,\qquad
\mathsf F\boldsymbol c=\mathsf T^t\mathsf N^{-1}\boldsymbol d
$$

In general, if you want to draw a sample from a multivariate distribution $\boldsymbol x\sim\mathcal N(\boldsymbol m,\mathsf C)$, you can produce it using

$$
\boldsymbol x=\boldsymbol\mu+\mathsf C^{1/2}\boldsymbol \eta
$$

where $\boldsymbol \eta\sim\mathcal(\boldsymbol 0,\mathsf I)$, which is quite easy to sample, as long as you have a routine to generate uniform random variates, and $\mathsf C^{1/2}$ is some non-unique matrix where $\mathsf C=(\mathsf C^{1/2})(\mathsf C^{1/2})^T$. We can verify this directly;

$$
\langle\boldsymbol x\rangle=\boldsymbol\mu+\mathsf C^{1/2}\langle\bolsymbol\eta\rangle
=\boldsymbol\mu
$$

and

$$
\langle\boldsymbol x\boldsymbol x^T\rangle
=\boldsymbol \mu\boldsymbol \mu^T
+\mathsf C^{1/2}\langle\boldsymbol\eta\boldsymbol\eta^T\rangle(\mathsf C^{1/2})^T
=\langle\boldsymbol\mu\rangle\boldsymbol\mu\rangle^T
+\mathsf C
$$



